import os
import traceback


def _apply_partitioning(df, target_partitions: str | None):
    if not target_partitions or not target_partitions.strip():
        return df
    try:
        n = int(target_partitions)
    except ValueError:
        return df

    # allow_repartition = os.getenv("SPARK_CLICKHOUSE_ALLOW_REPARTITION", "false").strip().lower() in (
    #     "1",
    #     "true",
    #     "yes",
    #     "y",
    # )
    # current = df.rdd.getNumPartitions()
    # if n < current:
    #     # ì…”í”Œ ì—†ì´ íŒŒí‹°ì…˜ ìˆ˜ë¥¼ ì¤„ì—¬ ì“°ê¸° ì˜¤ë²„í—¤ë“œë¥¼ ë‚®ì¶˜ë‹¤.
    #     return df.coalesce(n)
    # if n > current:
    #     if allow_repartition:
    #         # ë³‘ë ¬ ì“°ê¸°ë¥¼ ëŠ˜ë¦¬ê¸° ìœ„í•´ íŒŒí‹°ì…˜ì„ ì¬ë¶„ë°°í•œë‹¤.
    #         return df.repartition(n)
    #     print(
    #         "[â„¹ï¸ clickhouse sink] repartition ë¹„í™œì„±: "
    #         "SPARK_CLICKHOUSE_ALLOW_REPARTITION=trueë¡œ ì¼œì„¸ìš”."
    #     )
    return df


def write_to_clickhouse(
    df,
    table_name,
    batch_id: int | None = None,
    mode: str = "append",
):
    cached = False

    try:
        target_partitions = os.getenv("SPARK_CLICKHOUSE_WRITE_PARTITIONS")

        jdbc_batchsize = os.getenv("SPARK_CLICKHOUSE_JDBC_BATCHSIZE", "50000")
        clickhouse_url = os.getenv(
            "SPARK_CLICKHOUSE_URL",
            "jdbc:clickhouse://clickhouse:8123/analytics?compress=0&decompress=0&jdbcCompliant=false&async_insert=1&wait_for_async_insert=0",
            # async_insert=1 + wait_for_async_insert=0:
            # - Spark micro-batchì—ì„œ INSERT ì‘ë‹µ ëŒ€ê¸°ë¥¼ ì¤„ì—¬ ì²˜ë¦¬ëŸ‰ì„ ì˜¬ë¦°ë‹¤.
            # - ëŒ€ì‹œë³´ë“œ/ë¶„ì„ì€ MV ì§‘ê³„ í…Œì´ë¸”ë¡œ ë³´ë¯€ë¡œ, ì•½ê°„ì˜ ì§€ì—°ì€ í—ˆìš© ê°€ëŠ¥.
        )
        clickhouse_user = os.getenv("SPARK_CLICKHOUSE_USER", "log_user")
        clickhouse_password = os.getenv("SPARK_CLICKHOUSE_PASSWORD", "log_pwd")

        out_df = _apply_partitioning(df, target_partitions)

        writer = (
            out_df.write
            .format("jdbc") \
            .option("driver", "com.clickhouse.jdbc.ClickHouseDriver") \
            .option("url", clickhouse_url) \
            .option("user", clickhouse_user) \
            .option("password", clickhouse_password) \
            .option("dbtable", table_name) \
            .option("isolationLevel", "NONE") \
            .option("batchsize", jdbc_batchsize)
            .mode(mode)
        )
        writer.save()

    except Exception as e:
        print(f"[âŒ ERROR] ClickHouse ì €ì¥ ì‹¤íŒ¨: {table_name} {e}")
        msg = str(e)
        if "TABLE_ALREADY_EXISTS" in msg and "detached" in msg.lower():
            print(
                "[ğŸ› ï¸ ClickHouse] í…Œì´ë¸”ì´ DETACHED ìƒíƒœì…ë‹ˆë‹¤. ì•„ë˜ ëª…ë ¹ìœ¼ë¡œ ë³µêµ¬í•˜ì„¸ìš”:\n"
                "  sudo docker exec -it clickhouse clickhouse-client -u log_user --password log_pwd \\\n"
                f"    --query \"ATTACH TABLE {table_name}\""
            )
        traceback.print_exc()
    finally:
        try:
            if cached:
                out_df.unpersist()
        except Exception:
            pass
