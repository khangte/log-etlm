import os
import traceback


def _apply_partitioning(df, target_partitions: str | None):
    """apply_partitioning ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•œë‹¤."""
    if not target_partitions or not target_partitions.strip():
        return df
    try:
        n = int(target_partitions)
    except ValueError:
        return df

    allow_repartition = os.getenv("SPARK_CLICKHOUSE_ALLOW_REPARTITION", "false").strip().lower() in (
        "1",
        "true",
        "yes",
        "y",
    )
    current = df.rdd.getNumPartitions()
    if n < current:
        # ì…”í”Œ ì—†ì´ íŒŒí‹°ì…˜ ìˆ˜ë¥¼ ì¤„ì—¬ ì“°ê¸° ì˜¤ë²„í—¤ë“œë¥¼ ë‚®ì¶˜ë‹¤.
        return df.coalesce(n)
    if n > current:
        if allow_repartition:
            # ë³‘ë ¬ ì“°ê¸°ë¥¼ ëŠ˜ë¦¬ê¸° ìœ„í•´ íŒŒí‹°ì…˜ì„ ì¬ë¶„ë°°í•œë‹¤.
            return df.repartition(n)
        print(
            "[â„¹ï¸ clickhouse sink] repartition ë¹„í™œì„±: "
            "SPARK_CLICKHOUSE_ALLOW_REPARTITION=trueë¡œ ì¼œì„¸ìš”."
        )
    return df


def write_to_clickhouse(
    df,
    table_name,
    batch_id: int | None = None,
    mode: str = "append",
):
    """write_to_clickhouse ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•œë‹¤."""
    cached = False

    try:
        target_partitions = os.getenv("SPARK_CLICKHOUSE_WRITE_PARTITIONS")
        jdbc_batchsize = os.getenv("SPARK_CLICKHOUSE_JDBC_BATCHSIZE", "50000")
        clickhouse_url = os.getenv(
            "SPARK_CLICKHOUSE_URL",
            "jdbc:clickhouse://clickhouse:8123/analytics?compress=0&decompress=0&jdbcCompliant=false",
        )
        # async_insert=1ì´ ì´ í´ëŸ¬ìŠ¤í„°ì—ì„œ ClickHouse "Unknown error 1002"ë¥¼ ë°œìƒì‹œì¼œì„œ,
        # ë¹„ë™ê¸° ì“°ê¸°ê°€ ëª…ì‹œì ìœ¼ë¡œ ì§€ì›ë  ë•Œê¹Œì§€ ë™ê¸° íŒŒì´í”„ë¼ì¸ì„ ìœ ì§€í•©ë‹ˆë‹¤.
        clickhouse_user = os.getenv("SPARK_CLICKHOUSE_USER", "log_user")
        clickhouse_password = os.getenv("SPARK_CLICKHOUSE_PASSWORD", "log_pwd")

        out_df = _apply_partitioning(df, target_partitions)

        writer = (
            out_df.write
            .format("jdbc") \
            .option("driver", "com.clickhouse.jdbc.ClickHouseDriver") \
            .option("url", clickhouse_url) \
            .option("user", clickhouse_user) \
            .option("password", clickhouse_password) \
            .option("dbtable", table_name) \
            .option("isolationLevel", "NONE") \
            .option("batchsize", jdbc_batchsize)
            .mode(mode)
        )
        writer.save()

    except Exception as e:
        print(f"[âŒ ERROR] ClickHouse ì €ì¥ ì‹¤íŒ¨: {table_name} {e}")
        msg = str(e)
        if "TABLE_ALREADY_EXISTS" in msg and "detached" in msg.lower():
            print(
                "[ğŸ› ï¸ ClickHouse] í…Œì´ë¸”ì´ DETACHED ìƒíƒœì…ë‹ˆë‹¤. ì•„ë˜ ëª…ë ¹ìœ¼ë¡œ ë³µêµ¬í•˜ì„¸ìš”:\n"
                "  sudo docker exec -it clickhouse clickhouse-client -u log_user --password log_pwd \\\n"
                f"    --query \"ATTACH TABLE {table_name}\""
            )
        traceback.print_exc()
    finally:
        try:
            if cached:
                out_df.unpersist()
        except Exception:
            pass
